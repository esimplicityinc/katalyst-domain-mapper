---
id: ADR-015
title: "Server-Sent Events for Real-Time Agent Communication"
status: accepted
category: architecture
scope: "packages/intelligence/api, packages/intelligence/web"
created: "2026-02-17"
updated: "2026-02-17"
author: "Katalyst Team"
supersedes: ""
superseded_by: ""
---

# ADR-015: Server-Sent Events for Real-Time Agent Communication

## Status

**Accepted** - 2026-02-17

## Context

This artifact was created retroactively to document work completed on 2026-02-17.

Long-running AI agent operations are a core part of the Katalyst Intelligence platform. FOE scans take 5-15 minutes, governance validations take 30+ seconds, and interactive DDD discovery sessions involve multi-turn conversations with unpredictable response times. All of these operations had **no real-time progress feedback** — users stared at loading spinners with no indication of progress, status, or partial results.

### Problems with Previous Approach

1. **No progress visibility**: Users had no idea if a scan was 10% or 90% complete
2. **Timeout anxiety**: Long operations caused users to refresh the page, restarting the operation
3. **No streaming output**: Agent responses arrived as a single block after completion, missing the conversational feel of incremental token streaming
4. **Poor perceived performance**: Even fast operations felt slow without progressive feedback
5. **No error recovery**: If an agent failed mid-operation, the user only found out after the full timeout elapsed

### Alternatives Evaluated

**WebSockets**: Bidirectional communication protocol. Rejected because:
- Data flows one direction for agent output (server → client)
- Requires upgrade handshake, more complex server infrastructure
- Doesn't work through some corporate proxies without special configuration
- Overkill for unidirectional streaming

**Polling (short/long)**: Client repeatedly requests status updates. Rejected because:
- Short polling: Wasteful (many empty responses), adds latency (seconds between updates)
- Long polling: Complex to implement correctly, still has reconnection gaps
- Both: Higher server load than a single persistent connection
- Neither provides true real-time feedback

## Decision

Use **Server-Sent Events (SSE)** via the browser-native `EventSource` API for streaming agent output from server to client.

### Implementation Details

#### SSE Endpoint

```
GET /api/v1/stream/:sessionId
Content-Type: text/event-stream
Cache-Control: no-cache
Connection: keep-alive
```

#### Event Types

| Event | Purpose | Payload |
|-------|---------|---------|
| `agent:chunk` | Incremental text output from agent | `{ text: string, timestamp: number }` |
| `agent:status` | Status transition (thinking, executing, etc.) | `{ status: string, detail?: string }` |
| `agent:complete` | Operation finished successfully | `{ result: object, duration: number }` |
| `agent:error` | Operation failed | `{ error: string, code: string, recoverable: boolean }` |

#### Client Library: opencode-client.ts

```typescript
// Subscribe to agent events
const eventSource = subscribeToEvents(sessionId, {
  onChunk: (text) => appendToChat(text),
  onStatus: (status) => updateStatusBar(status),
  onComplete: (result) => handleResult(result),
  onError: (error) => handleError(error),
});

// Send user input via POST (separate from SSE stream)
await sendPromptAsync(sessionId, { prompt: userMessage });
```

#### Reconnection Strategy

- Browser-native `EventSource` auto-reconnects on connection drop
- Server sends `Last-Event-ID` headers for resumption
- Client sends `Last-Event-ID` on reconnect to receive missed events
- Heartbeat keepalive sent every 30 seconds to prevent proxy/CDN timeouts

#### Consumer Components

- **FOEChat.tsx** — FOE scanning interface with streaming agent output
- **DDDChat.tsx** — DDD discovery chat with incremental responses

Both components use the same SSE infrastructure via `opencode-client.ts`, ensuring consistent behavior across all agent interactions.

### Why SSE Over WebSockets

| Criterion | SSE | WebSockets |
|-----------|-----|------------|
| Direction | One-way (server → client) | Bidirectional |
| Protocol | HTTP/2 native | Requires upgrade handshake |
| Reconnection | Built-in (EventSource) | Manual implementation |
| Proxy compatibility | Works through standard HTTP | May require special config |
| Browser API | `EventSource` (simple) | `WebSocket` (more complex) |
| Complexity | Low | Medium-High |
| Our use case | Perfect fit (agent → UI) | Overkill |

User input (prompts, commands) is sent via standard `POST` requests, which is the correct semantic for client → server commands. SSE handles the server → client streaming. This separation of concerns is cleaner than multiplexing both directions over WebSockets.

### Why SSE Over Polling

| Criterion | SSE | Short Polling | Long Polling |
|-----------|-----|---------------|--------------|
| Latency | <100ms | 1-5 seconds | 0-timeout |
| Server load | 1 connection | N requests/min | N hold connections |
| Implementation | Simple | Simple | Complex |
| Real-time feel | Yes | No | Partial |
| Connection efficiency | Single persistent | Many short-lived | Many held open |

## Consequences

### Positive

1. **Real-time UX**: Agent output streams character-by-character, matching the conversational feel of modern AI chat interfaces
2. **Simple implementation**: `EventSource` API is 10 lines of client code; server-side is standard HTTP streaming
3. **Corporate proxy friendly**: SSE works over standard HTTPS without special proxy configuration
4. **Built-in reconnection**: `EventSource` handles dropped connections automatically with `Last-Event-ID` resumption
5. **HTTP/2 compatible**: No connection limit issues on modern infrastructure (unlimited streams per domain)
6. **Progressive feedback**: Users see agent thinking, executing, and producing results in real-time
7. **Shared infrastructure**: Both FOEChat and DDDChat use the same SSE client library

### Negative

1. **One-way only**: User input still goes via POST — two communication channels to manage
2. **Text-based data only**: SSE transmits text events, not binary data (not an issue for our use case)
3. **HTTP/1.1 connection limit**: Browsers limit to 6 SSE connections per domain on HTTP/1.1 (mitigated by HTTP/2 deployment)
4. **No backpressure**: If client can't keep up with events, they queue in memory (mitigated by reasonable chunk sizes)

### Risks

1. **Proxy/CDN buffering**: Some proxies buffer SSE responses — mitigated by `Cache-Control: no-cache` and `X-Accel-Buffering: no` headers, plus 30s heartbeat keepalive
2. **Memory accumulation on long sessions**: Extended agent conversations accumulate events in browser memory — mitigated by periodic DOM cleanup and message windowing in chat components

## Related

- **ROAD-031** — FOE Assessment Agent Chat Interface (primary consumer)
- **ROAD-015** — SSE Streaming for Real-time Agent Responses (foundational)
- **CAP-008** — Real-time Streaming capability
- **CAP-017** — AI Assessment Coaching (primary consumer of SSE)

---

**Template Version**: 1.0.0
**Last Updated**: 2026-02-17
